id: bq_to_gcs
namespace: ncaa_project
description: Python script to download and compress files to GCS bucket based on trigger which is on Moday every week at 3:00 AM. Then passing the value of date to dependant workflows.

tasks:
  - id: run_py
    type: io.kestra.plugin.scripts.python.Script
    description: Runs a basic Python script printing a message.
    taskRunner:
      type: io.kestra.plugin.core.runner.Process
    beforeCommands:
      - pip install --upgrade pip
      - pip install pyarrow==14.0.0
      - pip install "numpy<2"
      - pip install google-cloud-bigquery 
      - pip install google-cloud-storage 
      - pip install google-auth 
      - pip install pandas 
      - pip install db-dtypes
      - pip install google-cloud-bigquery-storage
    script: |
      from google.oauth2 import service_account
      from google.cloud import bigquery
      import google.auth
      from google.cloud import storage
      import math
      import io
      import os

      # Get google credentials downloaded by sftp as default
      credentials_path = os.environ.get("GCP_CREDENTIALS_PATH")
      credentials = service_account.Credentials.from_service_account_file(credentials_path)

      # TO DO add your projec id here
      project_id = "{{kv('GCP_PROJECT_ID')}}"

      client = bigquery.Client(credentials=credentials, project=project_id)
      storage_client = storage.Client(credentials=credentials, project=project_id)

      # Should be - bigquery-public-data, ncaa_basketball
      project_id = "bigquery-public-data"
      dataset_id = "ncaa_basketball"

      dataset_ref = client.dataset(dataset_id, project=project_id)
      tables = list(client.list_tables(dataset_ref))

      print(f"Found {len(tables)} tables in {project_id}.{dataset_id}")

      # Should be - ncaa-bucket-dump
      bucket_name = "{{kv('GCP_BUCKET_NAME')}}"
      bucket = storage_client.get_bucket(bucket_name)
      trigger_date = "{{trigger.date | date('dd-MM-yyyy')}}"
      # Runnning Query for each table and saving their parts in csv bucket
      for table in tables:

          table_name = table.table_id

          query_job = client.query(f"""
                                  SELECT *
                                  FROM `{project_id}.{dataset_id}.{table_name}`
                                  """)
          df = query_job.to_dataframe()
          
          print(f"table found: {table_name}")

          csv_buffer = io.BytesIO()
          # Write to Buffer
          compression_opts = dict(method="zip", archive_name=f"{table_name}_{trigger_date}.csv")
          df.to_csv(csv_buffer, index=False, compression=compression_opts)
          # Go to the start of the buffer and load to bucket
          csv_buffer.seek(0)  
          blob = bucket.blob(f"{table_name}/{table_name}_{trigger_date}.zip")
          blob.upload_from_file(csv_buffer)
          print(f"saved as {table_name}/{table_name}_{trigger_date}.zip")

triggers:
  - id: csv_schedule
    type: io.kestra.plugin.core.trigger.Schedule
    cron: "0 3 * * 1"

outputs:
  - id: pass_date
    type: STRING
    value: "{{trigger.date | date('dd-MM-yyyy')}}"
